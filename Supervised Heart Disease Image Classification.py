# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JHJ12sN2lpm83_9j33B0nZSkmthUsj9s

# Predict a possible heart disease

In this notebook, the main objective is to build a predictive model to see whether a patient would be diagnosed with heart disease. This model can help those with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease), an early detection and management of heart disease would be very helpful.

Not only that, we are also going to give insights of the data through visualization and graphing so that we can have an overview on the datasets.

Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.

People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help

## Import all neccessary libraries
"""

import os
import numpy as np
import pandas as pd
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
warnings.filterwarnings("ignore")
pd.set_option("display.max_rows",None)
from sklearn import preprocessing
import matplotlib
matplotlib.style.use('ggplot')
from sklearn.preprocessing import LabelEncoder

"""## 1.0 Data Understanding

#### 1.1 Reading dataset using Pandas library
"""

heart = pd.read_csv('heart.csv')
heart

heart.shape

"""From the above dataframe we can tell that,

- Our **target variable** on this dataset is the 'Heart Disease' column
- There are 918 rows and 12 columns

## 1.2 Data Description

In this section, we will get to know the all data types and its value

### 1.2.1 Understanding each columns

#### To know the data types of each columns
"""

heart.dtypes

"""There are some string data are saved as the form of object, so we have to convert it back to work on it"""

objectData = heart.select_dtypes(include="object").columns
heart[objectData]=heart[objectData].astype("string")

heart.dtypes

"""Now, all object data are converted into string format

#### Identifying the categorical and numerical columns
"""

#Identifying columns with categorical data
string_col=heart.select_dtypes("string").columns.to_list()
string_col

categorical_features = heart.columns.tolist()
categorical_features

#Identifying columns with numerical data excluding the target feature
num_col=heart.columns.to_list()
#remove columns with string data
for col in string_col:
    num_col.remove(col)
num_col.remove("HeartDisease")
print(num_col)

"""#### Understanding values of categorical columns"""

heart.Sex.unique()

heart.ChestPainType.unique()

heart.RestingECG.unique()

heart.ExerciseAngina.unique()

heart.ST_Slope.unique()

"""#### Understanding values of numerical columns"""

heart.FastingBS.unique()

"""From here, we know that FastingBS is a categorical data instead of numerical data even though its data type is int."""

heart.Age.unique()

heart.MaxHR.unique()

heart.Oldpeak.unique()

heart.RestingBP.unique()

"""We found that there are columns with zero values in 'RestingBP' which is impossible, therefore we will work on these columns during data preparation"""

heart.Cholesterol.unique()

"""We found that there are columns with zero values in 'Cholesterol' which is impossible, therefore we will work on these columns during data preparation

Numerical columns above are the columns we need to check for outliers in data preparation step

#### Checking for null value
"""

heart.isnull().sum()

"""From here, we know that there is no null values in this dataset. Therefore, there is no need to eliminate null values during data preparation step later.

#### Summary Statistics
"""

heart.describe().T

"""From here, we can list out all the attributes of this dataset
- Age: age of the patient [years]
- Sex: sex of the patient [M: Male, F: Female]
- ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]
- RestingBP: resting blood pressure [mm Hg]
- Cholesterol: serum cholesterol [mm/dl]
- FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]
- RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]
- MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]
- ExerciseAngina: exercise-induced angina [Y: Yes, N: No]
- Oldpeak: oldpeak = ST [Numeric value measured in depression]
- ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]
- HeartDisease: output class [1: heart disease, 0: Normal]

## 2.3 Exploratory Data Analysis

The purpose of this step :

- Understanding the given dataset and helps clean up the given dataset.
- It gives you a clear picture of the features and the relationships between them.
- Providing guidelines for essential variables and leaving behind/removing non-essential variables.
- Handling Missing values or human error.
- Identifying outliers.
- EDA process would be maximizing insights of a dataset.
- This process is time-consuming but very effective,

#### 2.3.1 Understanding average values of all the attributes for cases of with heart disease and withou heart disease
"""

yes = heart[heart['HeartDisease'] == 1].describe().T
no = heart[heart['HeartDisease'] == 0].describe().T
colors = ['#F93822','#FDD20E']

fig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (5,5))
plt.subplot(1,2,1)
sns.heatmap(yes[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f',)
plt.title('Heart Disease');

plt.subplot(1,2,2)
sns.heatmap(no[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')
plt.title('No Heart Disease');

fig.tight_layout(pad = 2)

"""## 2.3 Data Visualization

### Correlation Matrix wtih Heatmap
#### It's necessary to remove correlated variables to improve your model.One can find correlations using pandas “.corr()” function and can visualize the correlation matrix using plotly express.
- Lighter shades represents positive correlation
- Darker shades represents negative correlation
"""

px.imshow(heart.corr(),title="Correlation Plot of the Heat Failure Prediction")

"""Here we can see Heart Disease has a high negative correlation with "MaxHR" and somewhat negative correlation with "Cholesterol", where as here positive correatlation with "Oldpeak","FastingBS" and "RestingBP

### Data Visualization with Histogram

- Visualizing the distribution of heart disease between genders
"""

fig=px.histogram(heart,
                 x="HeartDisease",
                 color="Sex",
                 hover_data=heart.columns,
                 title="Distribution of Heart Diseases",
                 barmode="group")
fig.show()

"""From here, we can know that male have a higher chance of getting heart disease when compared to female

- Visualizing distribution of chest pain type with respect to genders
"""

fig=px.histogram(heart,
                 x="ChestPainType",
                 color="Sex",
                 hover_data=heart.columns,
                 title="Types of Chest Pain",
                 barmode = "stack")
fig.show()

# grouped = heart.groupby(['ChestPainType', 'Sex']).size().unstack(fill_value=0)

# grouped.plot(kind='bar', stacked=False)
# plt.title("Types of Chest Pain")
# plt.show()

"""From here, we can see that most of the male is having 'ASY' chest pain and most female is having 'ATA' chest pain"""

# fig=px.histogram(heart,
#                  x="Sex",
#                  hover_data=heart.columns,
#                  title="Sex Ratio in the Data")
# fig.show()

fig=px.histogram(heart,
                 x="RestingECG",
                 hover_data=heart.columns,
                 title="Distribution of Resting ECG")
fig.show()

"""#### Distribution of gender with pie chart"""

fig=px.pie(heart,
           names='Sex',
           title='Sex Ratio in the Data',
           color='Sex',
           color_discrete_map = {'M':'lightblue', 'F':'pink'})
fig.show()

"""Most of the respondents in this dataset are males

#### Distribution of Age for patients with and without heart disease
"""

fig = px.box(heart, x='HeartDisease', y='Age', title='Age Distribution by Heart Disease Status')
fig.show()

"""#### Relationship between Age and MaxHR with presence of HeartDisease"""

fig = px.scatter(heart, x='Age',
                 y='MaxHR',
                 color='HeartDisease',
                 title='Age vs. MaxHR by Heart Disease Status')
fig.show()

"""# 3.0 Data Preparation

### 3.1 Handling null values
"""

heart.isnull().sum()

"""Since there is no null values in the dataset no data cleaning is needed"""

heartCleaned = heart.copy()

"""### 3.2 Handling Outliers

From section 1.2.1, we know that numerical data are 'Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak'

We have to detect outliers for numerical data only by using boxplot
"""

#check outlier using boxplots
lst = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
fig= plt.figure(figsize=(4,4))
for i in lst:
    sns.boxplot(heart[i],palette='muted')
    plt.show()

"""There are lots of outliers in RestingBP and Cholesterol"""

heart['RestingBP'].describe()

heart['Cholesterol'].describe()

"""It is impossible to have 0 Cholesterol and 0 RestingBP"""

heart['MaxHR'].describe()

heart['Oldpeak'].describe()

"""Based on the box plot and .describe(), we can see that outliers has been detected. It is known that for these features having a maximum values much more larger than 75% quartile.

However, depending on the context of heart failure and our analysis purpose, we will only impute the outliers with zero values with their median, as the other outliers may contain valueble information and should not be removed
"""

#RestingBP

## Checking the number of 0 present in the RestingBP
RestingBP = heart[heart['RestingBP'] == 0]
RestingBP.shape

"""Only 1 row is having the RestingBP of 0"""

#RestingBP represents the blood pressure of the patient.
#It is not possible to have values equal to Zero(0).
# remove the value Zero(0)
heart = heart.drop(heart[(heart['RestingBP'] == 0)].index)
heart['RestingBP']

# Checking the number of 0 present in the Cholesterol
Cholesterol = heart[heart['Cholesterol'] == 0]
Cholesterol.shape

"""There are 171 rows having the Cholesterol of 0, we have to replace the zeros with median."""

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(missing_values=0, strategy='median')
imputer = imputer.fit(heart[['Cholesterol']])
heart['Cholesterol'] = imputer.transform(heart[['Cholesterol']])

# set Cholesterol data that has zero values to null
# heart.loc[heart['Cholesterol'] == 0,'Cholesterol'] = np.nan

# filling null value with median value of Cholesterol
# heart['Cholesterol'].fillna(heart['Cholesterol'].median,inplace = True)

heart['Cholesterol'].max

#dataset after outlier is cleared
heartClearOutlier = heart.copy()

"""## 3.3 Label Encoding

To handle categorical data including ordinal and nominal data
- One - Hot Encoding is suitable for nominal data with a small no of unique values [For working with non-tree based algortihms]
- Label Encoding is suitable for ordinal data with a small no of unique values  [For working with non-tree based algortihms]

In this project, we will use both tree-based and non-tree based algorithms.
Therefore, we will apply both label encoding method and use the encoded dataset accordingly.

### 3.3.1 One-Hot Encoding for non-tree based algorithms
"""

df_nontree=pd.get_dummies(heart,columns=string_col,drop_first=False)
df_nontree.head()

# Getting the target column at the end
target="HeartDisease"
y=df_nontree[target].values
df_nontree.drop("HeartDisease",axis=1,inplace=True)
df_nontree=pd.concat([df_nontree,heart[target]],axis=1)
df_nontree.head()

"""### 3.3.2 Label Encoding for tree based algorithms"""

df_tree = heart.copy()
encoder = LabelEncoder()
for col in string_col:
    df_tree[col] = encoder.fit_transform(df_tree[col])
df_tree.head()

"""## Data Preprosessing

### 3.4 Data Spliting

### Label encoded data (for tree-based algorithms)
"""

# feature selection - drop our target feature (Response) - our x input
df_tree_without_target_col = np.array (df_tree.drop('HeartDisease', axis = 1))

# create our targeted feature(Response) array - our y output
tree_trainHeartDiseaseData = np.array (df_tree['HeartDisease'], dtype = 'int64')


tree_trainData = df_tree_without_target_col

X_tree = tree_trainData
y_tree = tree_trainHeartDiseaseData

from sklearn.model_selection import train_test_split

X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(X_tree,
                                                                        y_tree,
                                                                        test_size = 0.2,
                                                                        random_state = 40)
#using the random state 40, we split the data into 80:20 for training:test

"""### One-Hot Encoded data (for non-tree based algorithms)"""

# feature selection - drop our target feature (Response) - our x input
df_nontree_without_target_col = np.array (df_nontree.drop('HeartDisease', axis = 1))

# create our targeted feature(Response) array - our y output
nontree_trainHeartDiseaseData = np.array (df_nontree['HeartDisease'], dtype = 'int64')


nontree_trainData = df_nontree_without_target_col

X_nontree = nontree_trainData
y_nontree = nontree_trainHeartDiseaseData

from sklearn.model_selection import train_test_split

X_train_nontree, X_test_nontree, y_train_nontree, y_test_nontree = train_test_split(X_nontree,
                                                                                    y_nontree,
                                                                                    test_size = 0.2,
                                                                                    random_state = 40)
#using the random state 40, we split the data into 80:20 for training:test

"""## 4.0 Modeling

In this modelling stage, we will be testing the dataset with four major models:

1. **KNN (K-Nearest Neighbour Classifier)** _requires feature scaling_
2. **SVM** _requires feature scaling_
3. **Decision Tree**
4. **Random Forest**
"""

#import libraries for model evaluation
from sklearn.metrics import plot_confusion_matrix,roc_auc_score, roc_curve, f1_score, accuracy_score,classification_report
from sklearn.metrics import make_scorer, precision_score, precision_recall_curve, plot_precision_recall_curve
from sklearn.metrics import recall_score, plot_roc_curve

import warnings
warnings.filterwarnings('ignore')

plt.rcParams['figure.figsize'] = (14,8)
plt.rcParams['figure.facecolor'] = '#F0F8FF'
plt.rcParams['figure.titlesize'] = 'medium'
plt.rcParams['figure.dpi'] = 100
plt.rcParams['figure.edgecolor'] = 'green'
plt.rcParams['figure.frameon'] = True

plt.rcParams["figure.autolayout"] = True

plt.rcParams['axes.facecolor'] = '#F5F5DC'
plt.rcParams['axes.titlesize'] = 25
plt.rcParams["axes.titleweight"] = 'normal'
plt.rcParams["axes.titlecolor"] = 'Olive'
plt.rcParams['axes.edgecolor'] = 'pink'
plt.rcParams["axes.linewidth"] = 2
plt.rcParams["axes.grid"] = True
plt.rcParams['axes.titlelocation'] = 'center'
plt.rcParams["axes.labelsize"] = 20
plt.rcParams["axes.labelpad"] = 2
plt.rcParams['axes.labelweight'] = 1
plt.rcParams["axes.labelcolor"] = 'Olive'
plt.rcParams["axes.axisbelow"] = False
plt.rcParams['axes.xmargin'] = .2
plt.rcParams["axes.ymargin"] = .2


plt.rcParams["xtick.bottom"] = True
plt.rcParams['xtick.color'] = '#A52A2A'
plt.rcParams["ytick.left"] = True
plt.rcParams['ytick.color'] = '#A52A2A'

plt.rcParams['axes.grid'] = True
plt.rcParams['grid.color'] = 'green'
plt.rcParams['grid.linestyle'] = '--'
plt.rcParams['grid.linewidth'] = .5
plt.rcParams['grid.alpha'] = .3

plt.rcParams['legend.loc'] = 'best'
plt.rcParams['legend.facecolor'] =  'NavajoWhite'
plt.rcParams['legend.edgecolor'] = 'pink'
plt.rcParams['legend.shadow'] = True
plt.rcParams['legend.fontsize'] = 20


plt.rcParams['font.family'] = 'Lucida Calligraphy'
plt.rcParams['font.size'] = 14

plt.rcParams['figure.dpi'] = 200
plt.rcParams['figure.edgecolor'] = 'Blue'

"""### Perform feature scaling for KNN and SVM models"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_X_train_nontree = scaler.fit_transform(X_train_nontree)
scaled_X_test_nontree = scaler.fit_transform(X_test_nontree)

"""### 1. KNN (K-Nearest Neighbour Classifier)

### Without feature scaling
"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(X_train_nontree, y_train_nontree)
y_pred_knn = knn.predict(X_test_nontree)

knn_train = round(knn.score(X_train_nontree, y_train_nontree) * 100, 2)
knn_accuracy = round(accuracy_score(y_pred_knn, y_test_nontree) * 100, 2)
knn_f1 = round(f1_score(y_pred_knn, y_test_nontree) * 100, 2)

print("Training Accuracy     :",knn_train,"%")
print("Model Accuracy Score  :",knn_accuracy,"%")
print("\033[1m--------------------------------------------------------\033[0m")
print("Classification_Report: \n",classification_report(y_test_nontree,y_pred_knn))

plot_confusion_matrix(knn, X_test_nontree, y_test_nontree);
plt.title('Confusion Matrix');

plot_roc_curve(knn, X_test_nontree, y_test_nontree);
plt.title('Roc Curve');

plot_precision_recall_curve(knn, X_test_nontree, y_test_nontree)
plt.title('Precision Recall Curve');

"""Here we are using k-fold cross-validation to evaluate the performance of the model. This involves splitting the data into k subsets and training the model on k-1 subsets while testing it on the remaining subset. This process is repeated k times, with each subset being used as the test set once.
We take the mean of the scores for each fold of the cross-validation(val_score) to get an estimate of the model’s accuracy.

"""

from sklearn.model_selection import cross_val_score
val_score = cross_val_score(estimator=knn, X = X_train_nontree, y=y_train_nontree, cv=10)
print("Model Accuracy Score: {:.2f} %".format(val_score.mean()*100))
print("Std. Dev: {:.2f} %".format(val_score.std()*100))

"""### With feature scaling"""

# from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(scaled_X_train_nontree, y_train_nontree)
y_pred_knn = knn.predict(scaled_X_test_nontree)

knn_train = round(knn.score(scaled_X_train_nontree, y_train_nontree) * 100, 2)
knn_accuracy = round(accuracy_score(y_pred_knn, y_test_nontree) * 100, 2)
knn_f1 = round(f1_score(y_pred_knn, y_test_nontree) * 100, 2)

print("Training Accuracy     :",knn_train,"%")
print("Model Accuracy Score  :",knn_accuracy,"%")
print("\033[1m--------------------------------------------------------\033[0m")
print("Classification_Report: \n",classification_report(y_test_nontree,y_pred_knn))

plot_confusion_matrix(knn, scaled_X_test_nontree, y_test_nontree);
plt.title('Confusion Matrix');

plot_roc_curve(knn, scaled_X_test_nontree, y_test_nontree);
plt.title('Roc Curve');

plot_precision_recall_curve(knn, scaled_X_test_nontree, y_test_nontree)
plt.title('Precision Recall Curve');

from sklearn.model_selection import cross_val_score
val_score = cross_val_score(estimator=knn, X = scaled_X_train_nontree, y=y_train_nontree, cv=10)
print("Model Accuracy Score: {:.2f} %".format(val_score.mean()*100))
print("Std. Dev: {:.2f} %".format(val_score.std()*100))

"""### 2. SVM

#### Without feature scaling
"""

# Support Vector Machines
from sklearn.svm import SVC
svc = SVC()
svc.fit(X_train_nontree, y_train_nontree)
y_pred_svc = svc.predict(X_test_nontree)

svc_train = round(svc.score(X_train_nontree, y_train_nontree) * 100, 2)
svc_accuracy = round(accuracy_score(y_pred_svc, y_test_nontree) * 100, 2)
svc_f1 = round(f1_score(y_pred_svc, y_test_nontree) * 100, 2)

print("Training Accuracy     :",svc_train,"%")
print("Model Accuracy Score  :",svc_accuracy,"%")
print("\033[1m--------------------------------------------------------\033[0m")
print("Classification_Report: \n",classification_report(y_test_nontree,y_pred_svc))

plot_confusion_matrix(svc, X_test_nontree, y_test_nontree);
plt.title('Confusion Matrix');

plot_roc_curve(svc, X_test_nontree, y_test_nontree);
plt.title('Roc Curve');

plot_precision_recall_curve(svc,  X_test_nontree, y_test_nontree)
plt.title('Precision Recall Curve');

from sklearn.model_selection import cross_val_score
val_score = cross_val_score(estimator=svc, X = X_train_nontree, y=y_train_nontree, cv=10)
print("Model Accuracy Score: {:.2f} %".format(val_score.mean()*100))
print("Std. Dev: {:.2f} %".format(val_score.std()*100))

"""### With feature scaling"""

# Support Vector Machines
from sklearn.svm import SVC
svc = SVC()
svc.fit(scaled_X_train_nontree, y_train_nontree)
y_pred_svc = svc.predict(scaled_X_test_nontree)

svc_train = round(svc.score(scaled_X_train_nontree, y_train_nontree) * 100, 2)
svc_accuracy = round(accuracy_score(y_pred_svc, y_test_nontree) * 100, 2)
svc_f1 = round(f1_score(y_pred_svc, y_test_nontree) * 100, 2)

print("Training Accuracy     :",svc_train,"%")
print("Model Accuracy Score  :",svc_accuracy,"%")
print("\033[1m--------------------------------------------------------\033[0m")
print("Classification_Report: \n",classification_report(y_test_nontree,y_pred_svc))

plot_confusion_matrix(svc, scaled_X_test_nontree, y_test_nontree);
plt.title('Confusion Matrix');

plot_roc_curve(svc, scaled_X_test_nontree, y_test_nontree);
plt.title('Roc Curve');

plot_precision_recall_curve(svc,  scaled_X_test_nontree, y_test_nontree)
plt.title('Precision Recall Curve');

from sklearn.model_selection import cross_val_score
val_score = cross_val_score(estimator=svc, X = scaled_X_train_nontree, y=y_train_nontree, cv=10)
print("Model Accuracy Score: {:.2f} %".format(val_score.mean()*100))
print("Std. Dev: {:.2f} %".format(val_score.std()*100))

"""It is very obvious that feature scaling is necessary for Support Vector Machines as the model accuracy score of modeling with feature scaling(84.73 %) is much higher than modeling without feature scaling(68.76 %)

### 3. Decision Tree
"""

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
decision = DecisionTreeClassifier()
decision.fit(X_train_tree, y_train_tree)
y_pred_Decision = decision.predict(X_test_tree)

decision_train = round(decision.score(X_train_tree, y_train_tree) * 100, 2)
decision_accuracy = round(accuracy_score(y_pred_Decision, y_test_tree) * 100, 2)
decision_f1 = round(f1_score(y_pred_Decision, y_test_tree) * 100, 2)

print("Training Accuracy     :",decision_train,"%")
print("Model Accuracy Score  :",decision_accuracy,"%")
print("\033[1m--------------------------------------------------------\033[0m")
print("Classification_Report: \n",classification_report(y_test_tree,y_pred_Decision))

plot_confusion_matrix(decision, X_test_tree, y_test_tree);
plt.title('Confusion Matrix');

plot_roc_curve(decision, X_test_tree, y_test_tree);
plt.title('Roc Curve');

plot_precision_recall_curve(decision, X_test_tree, y_test_tree)
plt.title('Precision Recall Curve');

from sklearn.model_selection import cross_val_score
val_score = cross_val_score(estimator=decision, X = X_train_tree, y=y_train_tree, cv=10)
print("Model Accuracy Score: {:.2f} %".format(val_score.mean()*100))
print("Std. Dev: {:.2f} %".format(val_score.std()*100))

"""### 4. Random Forest"""

# Random Forest
from sklearn.ensemble import RandomForestClassifier
random_forest = RandomForestClassifier(n_estimators=100)
random_forest.fit(X_train_tree, y_train_tree)
y_pred_random = random_forest.predict(X_test_tree)
random_forest.score(X_train_tree, y_train_tree)

random_forest_train = round(random_forest.score(X_train_tree, y_train_tree) * 100, 2)
random_forest_accuracy = round(accuracy_score(y_pred_random, y_test_tree) * 100, 2)
random_forest_f1 = round(f1_score(y_pred_random, y_test_tree) * 100, 2)

print("Training Accuracy     :",random_forest_train,"%")
print("Model Accuracy Score  :",random_forest_accuracy,"%")
print("\033[1m--------------------------------------------------------\033[0m")
print("Classification_Report: \n",classification_report(y_test_tree,y_pred_random))

plot_confusion_matrix(random_forest, X_test_tree, y_test_tree);
plt.title('Confusion Matrix');

plot_roc_curve(random_forest, X_test_tree, y_test_tree);
plt.title('Roc Curve');

plot_precision_recall_curve(random_forest, X_test_tree, y_test_tree)
plt.title('Precision Recall Curve');

from sklearn.model_selection import cross_val_score
val_score = cross_val_score(estimator=random_forest, X = X_train_tree, y=y_train_tree, cv=10)
print("Model Accuracy Score: {:.2f} %".format(val_score.mean()*100))
print("Std. Dev: {:.2f} %".format(val_score.std()*100))

"""We can get model accuracy score from accuracy_score and mean of cross_val_score.However, the two scores can be different because they are calculated using different formulas and different data.
The accuracy_score function calculates the accuracy based on the predicted labels and the true labels, while cross_val_score calculates the accuracy based on cross-validation.

"""

prediction1 = random_forest.predict(X_test_tree)
print(prediction1)

cross_checking = pd.DataFrame({'Actual' : y_test_tree , 'Predicted' : prediction1})
cross_checking.sample(15).style.background_gradient(
        cmap='coolwarm').set_properties(**{
            'font-family': 'Lucida Calligraphy',
            'color': 'LigntGreen',
            'font-size': '15px'
        })

"""# 5.0 Model Evaluation"""

models = pd.DataFrame({
    'Model': [
        'KNeighborsClassifier','Support Vector Machines',
        'Decision Tree','Random Forest'],
    'Training Accuracy': [
         knn_train,svc_train,
        decision_train, random_forest_train],
    'Model f1 Score': [
        knn_f1,svc_f1,
        decision_f1, random_forest_f1],
    'Model Accuracy Score': [
        knn_accuracy,svc_accuracy,
       decision_accuracy, random_forest_accuracy]

})

# Accuracy Comparison Table
models.sort_values(
    by='Model Accuracy Score', ascending=False).style.background_gradient(
        cmap='cool').hide_index().set_properties(**{
            'font-family': 'Lucida Calligraphy',
            'color': 'LigntGreen',
            'font-size': '15px'
        })

plt.rcParams['figure.figsize'] = (14,8)
plt.rcParams['font.size'] = 10
plt.rcParams['axes.xmargin'] = .2
plt.rcParams["axes.ymargin"] = .2


import seaborn as sns

sns.barplot(y= 'Model', x= 'Model Accuracy Score', data= models)
plt.title('COMPARE THE MODEL')
plt.xlabel('MODEL')
plt.ylabel('Training Accuracy');

"""We can see that Support Vector Machines have the highest training accuracy"""